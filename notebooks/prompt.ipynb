{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770acacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pydantic\n",
    "import os\n",
    "\n",
    "# Import the models and dataset\n",
    "from dataset.sudoku import SudokuDataset, SudokuAdapter\n",
    "\n",
    "# Configuration classes\n",
    "class HRMConfig(pydantic.BaseModel):\n",
    "    input_dim: int = 512\n",
    "    hidden_dim: int = 512\n",
    "    num_layers: int = 4\n",
    "    dropout: float = 0.1\n",
    "    output_dim: int = 512\n",
    "    N: int = 2  # number of high-level module cycles\n",
    "    T: int = 4  # number of low-level module cycles\n",
    "    max_seq_len: int = 81\n",
    "\n",
    "class ModelConfig(pydantic.BaseModel):\n",
    "    learning_rate: float = 0.001\n",
    "    batch_size: int = 32\n",
    "    max_epochs: int = 50\n",
    "    weight_decay: float = 0.01\n",
    "    patience: int = 10\n",
    "\n",
    "# Recurrent Module for HRM\n",
    "class RecurrentModule(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 128, num_layers: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.LSTM(input_size=input_dim, hidden_size=input_dim,\n",
    "                   num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.projection = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        for layer in self.layers:\n",
    "            x, hidden = layer(x, hidden)\n",
    "        \n",
    "        output = self.layer_norm(x)\n",
    "        output = self.projection(output)\n",
    "        return output\n",
    "\n",
    "# Hierarchical Reasoning Model\n",
    "class HierarchicalReasoningModel(nn.Module):\n",
    "    def __init__(self, config: HRMConfig, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.total_steps = config.N * config.T\n",
    "        self.device = device\n",
    "        self.N = config.N\n",
    "        self.T = config.T\n",
    "\n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(config.input_dim, config.hidden_dim)\n",
    "\n",
    "        # High and low level networks\n",
    "        self.High_net = RecurrentModule(\n",
    "            input_dim=config.input_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "\n",
    "        self.Low_net = RecurrentModule(\n",
    "            input_dim=config.input_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "\n",
    "        # Combine and project to latent\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_dim * 2)\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim * 2, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_dim // 2, config.output_dim)\n",
    "        )\n",
    "\n",
    "        # Projections\n",
    "        self.low_level_proj = nn.Linear(config.hidden_dim, config.hidden_dim, bias=False)\n",
    "        self.high_level_proj = nn.Linear(config.hidden_dim, config.hidden_dim, bias=False)\n",
    "\n",
    "    def initialize_hidden_states(self, batch_size: int):\n",
    "        z0_L = torch.zeros(batch_size, 1, self.config.hidden_dim, device=self.device)\n",
    "        z0_H = torch.zeros(batch_size, 1, self.config.hidden_dim, device=self.device)\n",
    "        return z0_H, z0_L\n",
    "\n",
    "    def level_step(self, first_level, second_level, input_embedding, network, projection):\n",
    "        level_influence = projection(second_level)\n",
    "        combined = first_level + level_influence + input_embedding\n",
    "        for layer in network.layers:\n",
    "            combined, _ = layer(combined)\n",
    "        return combined\n",
    "\n",
    "    def forward(self, x, hidden_states=None):\n",
    "        # x: (B, 81, input_dim)\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        if hidden_states is None:\n",
    "            high_level_state, low_level_state = self.initialize_hidden_states(x.shape[0])\n",
    "        else:\n",
    "            high_level_state, low_level_state = hidden_states\n",
    "\n",
    "        # Multi-step reasoning with gradient checkpointing\n",
    "        with torch.no_grad():\n",
    "            for step in range(self.total_steps - 1):\n",
    "                low_level_state = self.level_step(\n",
    "                    low_level_state, high_level_state, x, self.Low_net, self.low_level_proj\n",
    "                )\n",
    "\n",
    "                if (step + 1) % self.T == 0:\n",
    "                    high_level_state = self.level_step(\n",
    "                        high_level_state, low_level_state, x, self.High_net, self.high_level_proj\n",
    "                    )\n",
    "\n",
    "        # Final step with gradient\n",
    "        low_level_state = self.level_step(\n",
    "            low_level_state, high_level_state, x, self.Low_net, self.low_level_proj\n",
    "        )\n",
    "        high_level_state = self.level_step(\n",
    "            high_level_state, low_level_state, x, self.High_net, self.high_level_proj\n",
    "        )\n",
    "\n",
    "        # Combine both levels\n",
    "        combined = torch.cat([low_level_state, high_level_state], dim=-1)\n",
    "        combined = self.layer_norm(combined)\n",
    "\n",
    "        latent = self.output_proj(combined)  # (B, 81, output_dim)\n",
    "        return latent\n",
    "\n",
    "print(\"‚úÖ Model architectures loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199881b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training execution\n",
    "def main_training_pipeline():\n",
    "    \"\"\"Complete training pipeline for SudokuAdapter and HRM model\"\"\"\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "    \n",
    "    # Check if we have existing data\n",
    "    data_path = \"../data/sudoku_dataset.npy\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"üìä No existing dataset found. Generating new Sudoku dataset...\")\n",
    "        # Generate dataset using our previous function\n",
    "        dataset = generate_sudoku_field(1000, 5)  # 1000 samples, difficulty 5\n",
    "        np.save(data_path, dataset)\n",
    "        print(f\"üíæ Dataset saved to {data_path}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"üìÅ Loading Sudoku dataset...\")\n",
    "    puzzles, solutions = load_sudoku_data(data_path, max_samples=1000)\n",
    "    print(f\"üìä Loaded {len(puzzles)} puzzle-solution pairs\")\n",
    "    \n",
    "    # Create train/test split\n",
    "    train_puzzles, train_solutions, test_puzzles, test_solutions = create_train_test_split(\n",
    "        puzzles, solutions, test_size=0.2\n",
    "    )\n",
    "    \n",
    "    print(f\"üîÑ Data split: {len(train_puzzles)} training, {len(test_puzzles)} testing\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SudokuDataset(train_puzzles, train_solutions)\n",
    "    test_dataset = SudokuDataset(test_puzzles, test_solutions)\n",
    "    \n",
    "    # Initialize configurations\n",
    "    hrm_config = HRMConfig(\n",
    "        input_dim=512,\n",
    "        output_dim=512,\n",
    "        hidden_dim=512,\n",
    "        num_layers=4,\n",
    "        dropout=0.1,\n",
    "        N=2,\n",
    "        T=4\n",
    "    )\n",
    "    \n",
    "    model_config = ModelConfig(\n",
    "        learning_rate=0.001,\n",
    "        batch_size=32,\n",
    "        max_epochs=30,\n",
    "        weight_decay=0.01,\n",
    "        patience=10\n",
    "    )\n",
    "    \n",
    "    print(\"üèóÔ∏è  Initializing models...\")\n",
    "    \n",
    "    # Initialize models\n",
    "    model = HierarchicalReasoningModel(config=hrm_config, device=device)\n",
    "    adapter = SudokuAdapter(hidden_dim=256, hrm_input_dim=512, hrm_output_dim=512)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = HRMTrainer(model, adapter, config=model_config, device=device)\n",
    "    \n",
    "    print(f\"üéØ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"üéØ Adapter parameters: {sum(p.numel() for p in adapter.parameters()):,}\")\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\\\nüöÄ Starting training process...\")\n",
    "    trainer.train(train_dataset, val_dataset=test_dataset)\n",
    "    \n",
    "    # Plot training history\n",
    "    print(\"\\\\nüìà Plotting training history...\")\n",
    "    trainer.plot_training_history()\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    if os.path.exists('../results/best_model.pt'):\n",
    "        print(\"üîÑ Loading best model for evaluation...\")\n",
    "        checkpoint = torch.load('../results/best_model.pt', map_location=device)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        adapter.load_state_dict(checkpoint['adapter'])\n",
    "        print(f\"‚úÖ Loaded model from epoch {checkpoint['epoch']} with val_loss {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    # Comprehensive evaluation\n",
    "    print(\"\\\\nüîç Running comprehensive evaluation...\")\n",
    "    metrics = evaluate_model(model, adapter, test_dataset, device)\n",
    "    \n",
    "    # Visualize some predictions\n",
    "    print(\"\\\\nüé® Visualizing predictions...\")\n",
    "    visualize_predictions(model, adapter, test_dataset, device, num_examples=3)\n",
    "    \n",
    "    return model, adapter, trainer, metrics\n",
    "\n",
    "# Alternative: Load pre-trained model if available\n",
    "def load_pretrained_model(device):\n",
    "    \"\"\"Load a pre-trained model if available\"\"\"\n",
    "    model_path = '../results/best_model.pt'\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"‚ùå No pre-trained model found. Please train the model first.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize configurations (must match training config)\n",
    "    hrm_config = HRMConfig(\n",
    "        input_dim=512,\n",
    "        output_dim=512,\n",
    "        hidden_dim=512,\n",
    "        num_layers=4,\n",
    "        dropout=0.1,\n",
    "        N=2,\n",
    "        T=4\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    model = HierarchicalReasoningModel(config=hrm_config, device=device)\n",
    "    adapter = SudokuAdapter(hidden_dim=256, hrm_input_dim=512, hrm_output_dim=512)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    adapter.load_state_dict(checkpoint['adapter'])\n",
    "    \n",
    "    model.to(device).eval()\n",
    "    adapter.to(device).eval()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded pre-trained model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"üìä Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    return model, adapter\n",
    "\n",
    "print(\"‚úÖ Training pipeline ready! Run main_training_pipeline() to start training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a363ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the training pipeline\n",
    "print(\"üéØ Starting SudokuAdapter + HRM Training Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Option 1: Train a new model\n",
    "model, adapter, trainer, metrics = main_training_pipeline()\n",
    "\n",
    "print(\"\\\\nüéâ Training completed!\")\n",
    "print(\"üìä Final Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
